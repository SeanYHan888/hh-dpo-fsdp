{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Win Rate Analysis: Bug Investigation\n",
                "\n",
                "This notebook investigates the discrepancy between `summarize_judged.py` (51.9% win rate) and the correct calculation (93.2% win rate).\n",
                "\n",
                "**Root Cause**: `summarize_judged.py` uses `winner` (raw A/B position) instead of `winner_model_vs_chosen` (actual semantic winner), ignoring the A/B shuffle."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from pathlib import Path\n",
                "from math import sqrt\n",
                "from collections import Counter"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def wilson_ci(k, n, z=1.96):\n",
                "    \"\"\"Wilson score confidence interval for binomial proportion.\"\"\"\n",
                "    if n == 0:\n",
                "        return (0.0, 0.0)\n",
                "    phat = k / n\n",
                "    denom = 1 + z**2 / n\n",
                "    center = (phat + z**2 / (2 * n)) / denom\n",
                "    margin = z * sqrt((phat * (1 - phat) + z**2 / (4 * n)) / n) / denom\n",
                "    return center - margin, center + margin\n",
                "\n",
                "\n",
                "def load_judged_jsonl(path):\n",
                "    \"\"\"Load all records from a judged JSONL file.\"\"\"\n",
                "    records = []\n",
                "    with open(path, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            if line.strip():\n",
                "                records.append(json.loads(line))\n",
                "    return records"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path to the judged file from root pipeline\n",
                "judged_file = Path(\"../beta=0.1/judged_policy_vs_chosen.jsonl\")\n",
                "records = load_judged_jsonl(judged_file)\n",
                "print(f\"Loaded {len(records)} records from {judged_file.name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ› Buggy Version (what `summarize_judged.py` does)\n",
                "\n",
                "Uses `winner` field directly, assuming A is always the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def summarize_buggy(records):\n",
                "    \"\"\"Buggy version: uses raw 'winner' field (a/b/tie).\"\"\"\n",
                "    win = lose = tie = 0\n",
                "    \n",
                "    for ex in records:\n",
                "        result = ex[\"winner\"]  # BUG: This is raw A/B, not model vs chosen!\n",
                "        \n",
                "        if result == \"a\":\n",
                "            win += 1   # Assumes A is always model - WRONG!\n",
                "        elif result == \"b\":\n",
                "            lose += 1  # Assumes B is always hh_chosen - WRONG!\n",
                "        else:\n",
                "            tie += 1\n",
                "    \n",
                "    effective = win + lose\n",
                "    win_rate = win / effective if effective > 0 else 0.0\n",
                "    ci_low, ci_high = wilson_ci(win, effective)\n",
                "    \n",
                "    return {\n",
                "        \"method\": \"BUGGY (raw A/B)\",\n",
                "        \"win\": win,\n",
                "        \"lose\": lose,\n",
                "        \"tie\": tie,\n",
                "        \"total\": len(records),\n",
                "        \"win_rate\": win_rate,\n",
                "        \"ci\": (ci_low, ci_high)\n",
                "    }\n",
                "\n",
                "buggy_result = summarize_buggy(records)\n",
                "print(f\"ðŸ› BUGGY Result (summarize_judged.py):\")\n",
                "print(f\"   Win / Lose / Tie: {buggy_result['win']} / {buggy_result['lose']} / {buggy_result['tie']}\")\n",
                "print(f\"   Win-rate: {buggy_result['win_rate']:.1%}\")\n",
                "print(f\"   95% CI: [{buggy_result['ci'][0]:.3f}, {buggy_result['ci'][1]:.3f}]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## âœ… Correct Version (uses `winner_model_vs_chosen`)\n",
                "\n",
                "Uses the properly mapped field that accounts for A/B shuffling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def summarize_correct(records):\n",
                "    \"\"\"Correct version: uses 'winner_model_vs_chosen' field.\"\"\"\n",
                "    win = lose = tie = 0\n",
                "    \n",
                "    for ex in records:\n",
                "        result = ex[\"winner_model_vs_chosen\"]  # CORRECT: Already mapped!\n",
                "        \n",
                "        if result == \"model\":\n",
                "            win += 1\n",
                "        elif result == \"chosen\":\n",
                "            lose += 1\n",
                "        else:\n",
                "            tie += 1\n",
                "    \n",
                "    effective = win + lose\n",
                "    win_rate = win / effective if effective > 0 else 0.0\n",
                "    ci_low, ci_high = wilson_ci(win, effective)\n",
                "    \n",
                "    return {\n",
                "        \"method\": \"CORRECT (model vs chosen)\",\n",
                "        \"win\": win,\n",
                "        \"lose\": lose,\n",
                "        \"tie\": tie,\n",
                "        \"total\": len(records),\n",
                "        \"win_rate\": win_rate,\n",
                "        \"ci\": (ci_low, ci_high)\n",
                "    }\n",
                "\n",
                "correct_result = summarize_correct(records)\n",
                "print(f\"âœ… CORRECT Result:\")\n",
                "print(f\"   Win / Lose / Tie: {correct_result['win']} / {correct_result['lose']} / {correct_result['tie']}\")\n",
                "print(f\"   Win-rate: {correct_result['win_rate']:.1%}\")\n",
                "print(f\"   95% CI: [{correct_result['ci'][0]:.3f}, {correct_result['ci'][1]:.3f}]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š Why the Bug Happens: A/B Shuffle Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze the shuffle distribution\n",
                "swapped_count = sum(1 for r in records if r.get('is_swapped', False))\n",
                "not_swapped_count = len(records) - swapped_count\n",
                "\n",
                "print(f\"A/B Position Shuffle Analysis:\")\n",
                "print(f\"  Not swapped (A=model, B=chosen): {not_swapped_count} ({not_swapped_count/len(records):.1%})\")\n",
                "print(f\"  Swapped (A=chosen, B=model): {swapped_count} ({swapped_count/len(records):.1%})\")\n",
                "print(f\"\\nWhen swapped=True, 'winner=b' means MODEL wins, but buggy code counts it as LOSS!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show examples where the bug causes wrong counting\n",
                "wrong_counts = []\n",
                "for r in records:\n",
                "    winner_ab = r['winner']\n",
                "    winner_mvc = r['winner_model_vs_chosen']\n",
                "    is_swapped = r.get('is_swapped', False)\n",
                "    \n",
                "    # Buggy logic: a=win, b=lose\n",
                "    buggy_says = 'win' if winner_ab == 'a' else ('lose' if winner_ab == 'b' else 'tie')\n",
                "    # Correct logic: model=win, chosen=lose\n",
                "    correct_says = 'win' if winner_mvc == 'model' else ('lose' if winner_mvc == 'chosen' else 'tie')\n",
                "    \n",
                "    if buggy_says != correct_says:\n",
                "        wrong_counts.append({\n",
                "            'winner_ab': winner_ab,\n",
                "            'winner_mvc': winner_mvc,\n",
                "            'is_swapped': is_swapped,\n",
                "            'buggy_says': buggy_says,\n",
                "            'correct_says': correct_says\n",
                "        })\n",
                "\n",
                "print(f\"Misclassified by buggy code: {len(wrong_counts)} / {len(records)} ({len(wrong_counts)/len(records):.1%})\")\n",
                "print(f\"\\nFirst 5 misclassified examples:\")\n",
                "for w in wrong_counts[:5]:\n",
                "    print(f\"  winner={w['winner_ab']}, is_swapped={w['is_swapped']}, \"\n",
                "          f\"actual={w['winner_mvc']} â†’ buggy says {w['buggy_says']}, should be {w['correct_says']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“ˆ Comparison Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "comparison = pd.DataFrame([\n",
                "    {\n",
                "        \"Method\": \"Buggy (summarize_judged.py)\",\n",
                "        \"Field Used\": \"winner (a/b)\",\n",
                "        \"Wins\": buggy_result['win'],\n",
                "        \"Losses\": buggy_result['lose'],\n",
                "        \"Ties\": buggy_result['tie'],\n",
                "        \"Win Rate\": f\"{buggy_result['win_rate']:.1%}\",\n",
                "        \"95% CI\": f\"[{buggy_result['ci'][0]:.3f}, {buggy_result['ci'][1]:.3f}]\"\n",
                "    },\n",
                "    {\n",
                "        \"Method\": \"Correct\",\n",
                "        \"Field Used\": \"winner_model_vs_chosen\",\n",
                "        \"Wins\": correct_result['win'],\n",
                "        \"Losses\": correct_result['lose'],\n",
                "        \"Ties\": correct_result['tie'],\n",
                "        \"Win Rate\": f\"{correct_result['win_rate']:.1%}\",\n",
                "        \"95% CI\": f\"[{correct_result['ci'][0]:.3f}, {correct_result['ci'][1]:.3f}]\"\n",
                "    }\n",
                "])\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"COMPARISON SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "comparison"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ”§ Compare with gpt_judge_hh_final results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load gpt_judge_hh_final summary\n",
                "gpt_judge_summary_path = Path(\"../gpt_judge_hh_final/results/summary-beta-0.1.json\")\n",
                "\n",
                "if gpt_judge_summary_path.exists():\n",
                "    with open(gpt_judge_summary_path) as f:\n",
                "        gpt_summary = json.load(f)\n",
                "    \n",
                "    print(\"gpt_judge_hh_final Results:\")\n",
                "    print(f\"  Total: {gpt_summary['total']}\")\n",
                "    for model, wins in gpt_summary['counts'].items():\n",
                "        win_rate = gpt_summary['win_rates'][model]\n",
                "        print(f\"  {model}: {wins} ({win_rate:.1%})\")\n",
                "else:\n",
                "    print(f\"File not found: {gpt_judge_summary_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## âœ… Conclusion\n",
                "\n",
                "The win rate discrepancy was caused by a **bug in `summarize_judged.py`**:\n",
                "\n",
                "| Issue | Description |\n",
                "|-------|-------------|\n",
                "| **Bug** | Uses `winner` (raw A/B) instead of `winner_model_vs_chosen` |\n",
                "| **Impact** | When A/B is swapped (~50% of cases), wins are counted as losses |\n",
                "| **Buggy Rate** | ~51.9% |\n",
                "| **Correct Rate** | ~93.2% |\n",
                "\n",
                "Both pipelines produce similar **correct** results (~93%) when interpreted properly."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
